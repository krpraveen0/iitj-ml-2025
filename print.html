<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title></title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title"></h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>Welcome to the Machine Learning course documentation.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="machine-learning-syllabus"><a class="header" href="#machine-learning-syllabus">Machine Learning Syllabus</a></h1>
<h2 id="evaluation"><a class="header" href="#evaluation">Evaluation</a></h2>
<ul>
<li>Minor Exam: 25%</li>
<li>Major Exam: 35%</li>
<li>Assignments: 40%</li>
</ul>
<h2 id="course-structure"><a class="header" href="#course-structure">Course Structure</a></h2>
<h3 id="fractal-i-supervised-learning"><a class="header" href="#fractal-i-supervised-learning">Fractal I: Supervised Learning</a></h3>
<ul>
<li>Introduction to AI and ML</li>
<li>Different Paradigms of Machine Learning</li>
<li>Bayesian Classification, Decision Trees</li>
<li>Ensemble methods: Bagging, Boosting, Stacking</li>
</ul>
<h3 id="fractal-ii-graphical-models-neural-networks-deep-learning"><a class="header" href="#fractal-ii-graphical-models-neural-networks-deep-learning">Fractal II: Graphical Models, Neural Networks, Deep Learning</a></h3>
<ul>
<li>Graphical Models: HMM, MaxEnt, CRF</li>
<li>Neural Networks: Perceptron, Backpropagation</li>
<li>Deep Learning: RNN, LSTM, GRU, Encoder-Decoder, Attention, Autoencoder, GAN</li>
</ul>
<h3 id="fractal-iii-unsupervised-learning--feature-selection"><a class="header" href="#fractal-iii-unsupervised-learning--feature-selection">Fractal III: Unsupervised Learning &amp; Feature Selection</a></h3>
<ul>
<li>Feature Selection &amp; Dimensionality Reduction (PCA, LDA, Evolutionary Algorithms)</li>
<li>Clustering: k-means, k-medoids, EM, Agglomerative</li>
<li>Hypothesis Evaluation, VC-dimension, Bias-Variance Tradeoff, Regression</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="evaluation-scheme"><a class="header" href="#evaluation-scheme">Evaluation Scheme</a></h1>
<ul>
<li>Minor Exam: 25%</li>
<li>Major Exam: 35%</li>
<li>Assignments: 40%</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="course-schedule--lecture-tracking"><a class="header" href="#course-schedule--lecture-tracking">Course Schedule &amp; Lecture Tracking</a></h1>
<div class="table-wrapper"><table><thead><tr><th>Week</th><th>Date</th><th>Topic</th><th>Status</th><th>Notes Link</th></tr></thead><tbody>
<tr><td>1</td><td>Aug 23, 2025</td><td>Introduction to AI &amp; ML</td><td>✅ Completed</td><td><a href="./notes/lecture1.html">Lecture 1</a></td></tr>
<tr><td>2</td><td>Aug 24, 2025</td><td>ML Paradigms, Bayesian, Decision Trees</td><td>✅ Completed</td><td><a href="./notes/lecture2.html">Lecture 2</a></td></tr>
<tr><td>3</td><td>Aug 30, 2025</td><td>Ensemble Methods</td><td>✅ Completed</td><td><a href="./notes/lecture3.html">Lecture 3</a></td></tr>
<tr><td>4</td><td>Aug 31, 2025</td><td>Graphical Models (HMM, CRF, MaxEnt)</td><td>✅ Completed</td><td><a href="./notes/lecture4.html">Lecture 4</a></td></tr>
<tr><td>5</td><td>Sep 6, 2025</td><td>Perceptron &amp; Backpropagation</td><td>🔜 Upcoming</td><td>-</td></tr>
<tr><td>6</td><td>Sep 7, 2025</td><td>Deep Learning (RNN, LSTM, GRU)</td><td>🔜 Upcoming</td><td>-</td></tr>
<tr><td>7</td><td>Sep 13, 2025</td><td>Encoder-Decoder, Attention, GANs</td><td>🔜 Upcoming</td><td>-</td></tr>
<tr><td>8</td><td>Sep 14, 2025</td><td>Feature Selection &amp; PCA</td><td>🔜 Upcoming</td><td>-</td></tr>
<tr><td>9</td><td>Sep 20, 2025</td><td>Clustering Methods</td><td>🔜 Upcoming</td><td>-</td></tr>
<tr><td>10</td><td>Sep 21, 2025</td><td>Hypothesis Evaluation &amp; Regression</td><td>🔜 Upcoming</td><td>-</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="course-resources"><a class="header" href="#course-resources">Course Resources</a></h1>
<h3 id="textbooks"><a class="header" href="#textbooks">Textbooks</a></h3>
<ul>
<li>T. Mitchell. <em>Machine Learning</em>. McGraw-Hill, 1997</li>
<li>Christopher Bishop. <em>Pattern Recognition and Machine Learning</em>. Springer, 2006</li>
<li>Hastie, Tibshirani, Friedman. <em>The Elements of Statistical Learning</em>. Springer</li>
</ul>
<h3 id="reference"><a class="header" href="#reference">Reference</a></h3>
<ul>
<li>Papoulis &amp; Pillai. <em>Probability, Random Variables and Stochastic Processes</em>, 4th Ed.</li>
<li>A. K. Jain &amp; R. C. Dubes. <em>Algorithms for Clustering Data</em>. Prentice Hall, 1988</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fractal-i-supervised-learning-1"><a class="header" href="#fractal-i-supervised-learning-1">Fractal I: Supervised Learning</a></h1>
<p>Overview and lecture notes for supervised learning.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-week-0-math-foundations-detailed-notes--problems"><a class="header" href="#-week-0-math-foundations-detailed-notes--problems">📘 Week 0: Math Foundations (Detailed Notes + Problems)</a></h1>
<hr />
<h2 id="-1-linear-algebra"><a class="header" href="#-1-linear-algebra">🔹 1. Linear Algebra</a></h2>
<h3 id="11-vectors"><a class="header" href="#11-vectors"><strong>1.1 Vectors</strong></a></h3>
<ul>
<li>A vector = ordered list of numbers.<br />
$$
\mathbf{x} = \begin{bmatrix} x_1 \ x_2 \ x_3 \end{bmatrix}
$$<br />
Example: A student’s marks in 3 subjects: [80, 75, 90].</li>
</ul>
<p>👉 Think of vectors as <strong>arrows</strong> in space pointing from the origin.</p>
<hr />
<h3 id="12-vector-operations"><a class="header" href="#12-vector-operations"><strong>1.2 Vector Operations</strong></a></h3>
<ul>
<li>
<p><strong>Addition</strong>:
$$
[1,2] + [3,4] = [4,6]
$$</p>
</li>
<li>
<p><strong>Scalar multiplication</strong>:
$$
2 \cdot [3,5] = [6,10]
$$</p>
</li>
<li>
<p><strong>Dot Product (similarity)</strong>:
$$
\mathbf{x}\cdot \mathbf{y} = \sum_{i=1}^n x_i y_i
$$<br />
Example:<br />
[1,2] ⋅ [3,4] = (1)(3) + (2)(4) = 11.</p>
</li>
<li>
<p><strong>Norm (Length of vector)</strong>:
$$
|\mathbf{x}| = \sqrt{x_1^2 + x_2^2 + \dots + x_n^2}
$$<br />
Example: ||[3,4]|| = √(3²+4²) = 5.</p>
</li>
</ul>
<hr />
<h3 id="13-matrices"><a class="header" href="#13-matrices"><strong>1.3 Matrices</strong></a></h3>
<p>A matrix is a <strong>table of numbers</strong>.<br />
$$
A = \begin{bmatrix} 1 &amp; 2 \ 3 &amp; 4 \end{bmatrix}
$$</p>
<ul>
<li>
<h1 bmatrix=""><strong>Matrix multiplication</strong>:<br />
Multiply row of first × column of second.<br />
$$
\begin{bmatrix} 1 &amp; 2 \ 3 &amp; 4 \end{bmatrix}
\cdot
\begin{bmatrix} 5 &amp; 6 \ 7 &amp; 8 \end</h1>
\begin{bmatrix} 19 &amp; 22 \ 43 &amp; 50 \end{bmatrix}
$$</li>
</ul>
<hr />
<h3 id="-solved-example"><a class="header" href="#-solved-example">✅ Solved Example</a></h3>
<p>Compute dot product and norm:<br />
$$
x = [2,3], \quad y = [4,5]
$$</p>
<ul>
<li>Dot product: 2·4 + 3·5 = 23.</li>
<li>Norm of x: √(2²+3²) = √13 ≈ 3.61.</li>
</ul>
<hr />
<h3 id="-practice-problems"><a class="header" href="#-practice-problems">💡 Practice Problems</a></h3>
<ol>
<li>Compute [1,2,3] ⋅ [4,5,6].</li>
<li>Find the length of vector [6,8].</li>
<li>Multiply matrices:<br />
$$\begin{bmatrix}1 &amp; 2 \ 0 &amp; 1\end{bmatrix} \cdot \begin{bmatrix}3 &amp; 4 \ 5 &amp; 6\end{bmatrix}$$</li>
</ol>
<hr />
<h2 id="-2-calculus"><a class="header" href="#-2-calculus">🔹 2. Calculus</a></h2>
<h3 id="21-derivatives"><a class="header" href="#21-derivatives"><strong>2.1 Derivatives</strong></a></h3>
<p>The derivative measures <strong>rate of change</strong> (slope of curve).<br />
$$
f'(x) = \frac{df}{dx}
$$</p>
<ul>
<li>Example:<br />
If f(x) = x², then f'(x) = 2x.<br />
At x=3, slope = 6.</li>
</ul>
<hr />
<h3 id="22-gradients"><a class="header" href="#22-gradients"><strong>2.2 Gradients</strong></a></h3>
<p>For functions with many variables:<br />
$$
f(x,y) = x^2 + y^2
$$<br />
Gradient = vector of partial derivatives:<br />
$$
\nabla f(x,y) = \left[ \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y} \right] = [2x, 2y]
$$</p>
<hr />
<h3 id="23-optimization-gradient-descent"><a class="header" href="#23-optimization-gradient-descent"><strong>2.3 Optimization (Gradient Descent)</strong></a></h3>
<p>ML uses calculus to <strong>minimize error</strong>.<br />
Update rule:
$$
\theta := \theta - \alpha \cdot \nabla J(\theta)
$$<br />
(where α = learning rate)</p>
<hr />
<h3 id="-solved-example-1"><a class="header" href="#-solved-example-1">✅ Solved Example</a></h3>
<p>Find derivative of:<br />
$$
f(x) = 3x^2 + 2x
$$<br />
Solution:<br />
f'(x) = 6x + 2.<br />
At x=2: slope = 6(2)+2 = 14.</p>
<hr />
<h3 id="-practice-problems-1"><a class="header" href="#-practice-problems-1">💡 Practice Problems</a></h3>
<ol>
<li>Differentiate f(x) = x³ - 5x² + 6.</li>
<li>Compute gradient of f(x,y) = x² + xy + y².</li>
<li>If f(x) = x⁴, find slope at x=2.</li>
</ol>
<hr />
<h2 id="-3-probability--statistics"><a class="header" href="#-3-probability--statistics">🔹 3. Probability &amp; Statistics</a></h2>
<h3 id="31-basics"><a class="header" href="#31-basics"><strong>3.1 Basics</strong></a></h3>
<ul>
<li>P(A): probability of event A.</li>
<li>P(A ∩ B): probability of A and B.</li>
<li>P(A|B): probability of A given B.</li>
</ul>
<p><strong>Bayes’ Rule</strong>:<br />
$$
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
$$</p>
<hr />
<h3 id="32-expectation--variance"><a class="header" href="#32-expectation--variance"><strong>3.2 Expectation &amp; Variance</strong></a></h3>
<ul>
<li>
<p><strong>Expectation</strong>:
$$
E[X] = \sum x_i P(x_i)
$$<br />
(average value)</p>
</li>
<li>
<p><strong>Variance</strong>:
$$
Var(X) = E[(X - E[X])^2]
$$<br />
(spread of data)</p>
</li>
</ul>
<hr />
<h3 id="33-common-distributions"><a class="header" href="#33-common-distributions"><strong>3.3 Common Distributions</strong></a></h3>
<ul>
<li><strong>Bernoulli</strong>: coin flip (0/1).</li>
<li><strong>Gaussian (Normal)</strong>: bell curve.</li>
</ul>
<hr />
<h3 id="-solved-example-2"><a class="header" href="#-solved-example-2">✅ Solved Example</a></h3>
<p>Roll a fair dice. What is probability of 6?<br />
$$
P(6) = \frac{1}{6} ≈ 0.1667
$$</p>
<p>If you roll dice 600 times, expected number of 6s = 600 × 1/6 = 100.</p>
<hr />
<h3 id="-practice-problems-2"><a class="header" href="#-practice-problems-2">💡 Practice Problems</a></h3>
<ol>
<li>Toss a fair coin 3 times. Find probability of getting 2 heads.</li>
<li>Compute mean &amp; variance of dataset: [2, 4, 4, 4, 5, 5, 7, 9].</li>
<li>Apply Bayes’ rule:<br />
A patient has a disease with probability 1%. A test is 90% accurate. If test is positive, what is probability that patient has disease?</li>
</ol>
<hr />
<h2 id="-4-python-practice"><a class="header" href="#-4-python-practice">🔹 4. Python Practice</a></h2>
<pre><code class="language-python">import numpy as np

# Linear Algebra
x = np.array([2,3]); y = np.array([4,5])
print("Dot product:", np.dot(x,y))
print("Norm of x:", np.linalg.norm(x))

# Calculus (sympy)
import sympy as sp
x,y = sp.symbols('x y')
f = 3*x**2 + 2*x
print("Derivative:", sp.diff(f, x))
g = x**2 + x*y + y**2
print("Gradient:", [sp.diff(g, var) for var in (x,y)])

# Probability
data = [2,4,4,4,5,5,7,9]
print("Mean:", np.mean(data))
print("Variance:", np.var(data))
</code></pre>
<hr />
<h2 id="-summary"><a class="header" href="#-summary">📌 Summary</a></h2>
<ul>
<li><strong>Linear Algebra</strong> = structure of data (vectors/matrices).</li>
<li><strong>Calculus</strong> = optimization (slopes, gradients).</li>
<li><strong>Probability</strong> = uncertainty, distributions, Bayes’ rule.</li>
</ul>
<hr />
<div style="break-before: page; page-break-before: always;"></div><h1 id="-lecture-1-introduction-to-machine-learning"><a class="header" href="#-lecture-1-introduction-to-machine-learning">📘 Lecture 1: Introduction to Machine Learning</a></h1>
<hr />
<h2 id="-1-what-is-machine-learning"><a class="header" href="#-1-what-is-machine-learning">🔹 1. What is Machine Learning?</a></h2>
<h3 id="definition-tom-mitchell-1997"><a class="header" href="#definition-tom-mitchell-1997"><strong>Definition (Tom Mitchell, 1997)</strong></a></h3>
<p>A computer program is said to <strong>learn</strong> from experience <strong>E</strong> with respect to some class of tasks <strong>T</strong> and performance measure <strong>P</strong>, if its performance at tasks in T, as measured by P, improves with experience E.</p>
<p>👉 Example:</p>
<ul>
<li><strong>Task (T)</strong>: Predict if tomorrow will be sunny or rainy.</li>
<li><strong>Performance (P)</strong>: Accuracy of predictions.</li>
<li><strong>Experience (E)</strong>: Past weather data (temperature, humidity, etc).</li>
</ul>
<hr />
<h2 id="-2-components-of-ml"><a class="header" href="#-2-components-of-ml">🔹 2. Components of ML</a></h2>
<ol>
<li>
<p><strong>Data Storage</strong> – Collect data (tables, images, text).</p>
<ul>
<li>Example: Students’ hours studied vs. exam marks.</li>
</ul>
</li>
<li>
<p><strong>Abstraction</strong> – Represent data with models.</p>
<ul>
<li>Example: Fit a line y = mx + b.</li>
</ul>
</li>
<li>
<p><strong>Generalization</strong> – Perform well on <strong>new unseen data</strong>, not just training data.</p>
<ul>
<li>Example: Model predicts marks of a new student.</li>
</ul>
</li>
<li>
<p><strong>Evaluation</strong> – Measure performance (accuracy, error rate, precision, recall, F1).</p>
</li>
</ol>
<hr />
<h2 id="-3-types-of-learning"><a class="header" href="#-3-types-of-learning">🔹 3. Types of Learning</a></h2>
<h3 id="1-supervised-learning"><a class="header" href="#1-supervised-learning"><strong>1. Supervised Learning</strong></a></h3>
<ul>
<li>Data has <strong>labels</strong>.</li>
<li>Goal: Learn mapping from inputs → outputs.</li>
<li>Examples:
<ul>
<li>Predict house prices (Regression).</li>
<li>Spam email detection (Classification).</li>
</ul>
</li>
</ul>
<h3 id="2-unsupervised-learning"><a class="header" href="#2-unsupervised-learning"><strong>2. Unsupervised Learning</strong></a></h3>
<ul>
<li>Data has <strong>no labels</strong>.</li>
<li>Goal: Find structure/patterns.</li>
<li>Examples:
<ul>
<li>Group customers by spending (Clustering).</li>
<li>Reduce image dimensions (PCA).</li>
</ul>
</li>
</ul>
<h3 id="3-reinforcement-learning"><a class="header" href="#3-reinforcement-learning"><strong>3. Reinforcement Learning</strong></a></h3>
<ul>
<li>Agent interacts with <strong>environment</strong>.</li>
<li>Learns by rewards and penalties.</li>
<li>Examples:
<ul>
<li>Training robots to walk.</li>
<li>AlphaGo playing chess/Go.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="-4-theoretical-perspectives"><a class="header" href="#-4-theoretical-perspectives">🔹 4. Theoretical Perspectives</a></h2>
<h3 id="version-spaces"><a class="header" href="#version-spaces"><strong>Version Spaces</strong></a></h3>
<ul>
<li>All hypotheses consistent with training data.</li>
<li>Example: If hypothesis = “Play Tennis when sunny,” then version space = all rules that fit given samples.</li>
</ul>
<hr />
<h3 id="pac-learning-probably-approximately-correct"><a class="header" href="#pac-learning-probably-approximately-correct"><strong>PAC Learning (Probably Approximately Correct)</strong></a></h3>
<ul>
<li>Introduced by Valiant (1984).</li>
<li>Idea: An algorithm is PAC-learnable if, with high probability, it outputs a hypothesis close to the true function, given enough training examples.</li>
<li>Formal: For any ε (accuracy) and δ (confidence), the learner finds h such that:<br />
$$
P(error(h) \leq \epsilon) \geq 1 - \delta
$$</li>
</ul>
<hr />
<h3 id="vc-dimension-vapnikchervonenkis"><a class="header" href="#vc-dimension-vapnikchervonenkis"><strong>VC Dimension (Vapnik–Chervonenkis)</strong></a></h3>
<ul>
<li>Measures <strong>capacity/complexity</strong> of a model.</li>
<li>Higher VC = more complex model (can fit more patterns).</li>
<li>Example:
<ul>
<li>A line in 2D can shatter (perfectly classify) at most 3 points → VC dimension = 3.</li>
<li>A perceptron in d dimensions has VC dimension = d+1.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="-5-coding-example--enjoy-sport-dataset"><a class="header" href="#-5-coding-example--enjoy-sport-dataset">🔹 5. Coding Example – “Enjoy Sport” Dataset</a></h2>
<p>We’ll build a <strong>rule-based classifier</strong>.</p>
<h3 id="dataset"><a class="header" href="#dataset">Dataset</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Weather</th><th>Temperature</th><th>Humidity</th><th>Wind</th><th>Play Sport</th></tr></thead><tbody>
<tr><td>Sunny</td><td>Hot</td><td>High</td><td>Weak</td><td>No</td></tr>
<tr><td>Sunny</td><td>Hot</td><td>High</td><td>Strong</td><td>No</td></tr>
<tr><td>Overcast</td><td>Hot</td><td>High</td><td>Weak</td><td>Yes</td></tr>
<tr><td>Rain</td><td>Mild</td><td>High</td><td>Weak</td><td>Yes</td></tr>
<tr><td>Rain</td><td>Cool</td><td>Normal</td><td>Weak</td><td>Yes</td></tr>
<tr><td>Rain</td><td>Cool</td><td>Normal</td><td>Strong</td><td>No</td></tr>
<tr><td>Overcast</td><td>Cool</td><td>Normal</td><td>Strong</td><td>Yes</td></tr>
<tr><td>Sunny</td><td>Mild</td><td>High</td><td>Weak</td><td>No</td></tr>
</tbody></table>
</div>
<hr />
<h3 id="python-implementation"><a class="header" href="#python-implementation">Python Implementation</a></h3>
<pre><code class="language-python"># Rule-based classifier for "Enjoy Sport"

# Simple dataset
data = [
    ("Sunny", "Hot", "High", "Weak", "No"),
    ("Sunny", "Hot", "High", "Strong", "No"),
    ("Overcast", "Hot", "High", "Weak", "Yes"),
    ("Rain", "Mild", "High", "Weak", "Yes"),
    ("Rain", "Cool", "Normal", "Weak", "Yes"),
    ("Rain", "Cool", "Normal", "Strong", "No"),
    ("Overcast", "Cool", "Normal", "Strong", "Yes"),
    ("Sunny", "Mild", "High", "Weak", "No"),
]

# Build simple rule-based model (enjoy sport if Overcast OR (Rain &amp; Weak Wind))
def rule_based_classifier(weather, temp, humidity, wind):
    if weather == "Overcast":
        return "Yes"
    if weather == "Rain" and wind == "Weak":
        return "Yes"
    return "No"

# Test classifier
correct = 0
for row in data:
    prediction = rule_based_classifier(*row[:-1])
    if prediction == row[-1]:
        correct += 1

print(f"Accuracy: {correct/len(data) * 100:.2f}%")
</code></pre>
<p>Expected output: <strong>Accuracy ~100%</strong> (since rules match dataset).</p>
<hr />
<h2 id="-practice-tasks"><a class="header" href="#-practice-tasks">🎯 Practice Tasks</a></h2>
<ol>
<li>Write your own rule-based classifier for “Play Tennis” dataset.</li>
<li>Identify T, P, and E for the task: predicting movie ratings from user history.</li>
<li>Give an example of supervised, unsupervised, and reinforcement learning from <strong>your daily life</strong>.</li>
<li>Draw version space for hypotheses: “Play Sport if Weather = Sunny OR Overcast.”</li>
<li>Compute VC dimension of:
<ul>
<li>A line in 1D.</li>
<li>A line in 2D.</li>
</ul>
</li>
</ol>
<hr />
<h2 id="-summary-1"><a class="header" href="#-summary-1">📝 Summary</a></h2>
<ul>
<li><strong>ML definition</strong> = Task, Performance, Experience.</li>
<li><strong>Components</strong> = data storage, abstraction, generalization, evaluation.</li>
<li><strong>Types</strong> = Supervised, Unsupervised, Reinforcement.</li>
<li><strong>Theories</strong> = Version Spaces, PAC learning, VC dimension.</li>
<li><strong>Hands-on</strong>: Built simple rule-based classifier for “Enjoy Sport.”</li>
</ul>
<hr />
<div style="break-before: page; page-break-before: always;"></div><h1 id="-lecture-2-ml-paradigms-bayesian-decision-trees"><a class="header" href="#-lecture-2-ml-paradigms-bayesian-decision-trees">📘 Lecture 2: ML Paradigms, Bayesian, Decision Trees</a></h1>
<hr />
<h2 id="1-ml-paradigms"><a class="header" href="#1-ml-paradigms">1. ML Paradigms</a></h2>
<ul>
<li><strong>Supervised Learning:</strong> Learn from labeled data (e.g., classification, regression).</li>
<li><strong>Unsupervised Learning:</strong> Find patterns in unlabeled data (e.g., clustering, dimensionality reduction).</li>
<li><strong>Reinforcement Learning:</strong> Learn by interacting with environment, receiving rewards/penalties.</li>
<li><strong>Semi-supervised Learning:</strong> Use both labeled and unlabeled data.</li>
<li><strong>Self-supervised Learning:</strong> Generate labels from data itself (e.g., predicting next word in a sentence).</li>
</ul>
<h2 id="2-bayesian-learning"><a class="header" href="#2-bayesian-learning">2. Bayesian Learning</a></h2>
<ul>
<li><strong>Bayes Theorem:</strong>
$$P(H|D) = \frac{P(D|H)P(H)}{P(D)}$$
<ul>
<li>$P(H|D)$: Posterior, $P(D|H)$: Likelihood, $P(H)$: Prior, $P(D)$: Evidence</li>
</ul>
</li>
<li><strong>Naive Bayes Classifier:</strong> Assumes features are conditionally independent given the class.</li>
<li><strong>MAP (Maximum a Posteriori):</strong> Choose hypothesis with highest posterior probability.</li>
<li><strong>ML (Maximum Likelihood):</strong> Choose hypothesis that maximizes likelihood of data.</li>
</ul>
<h2 id="3-decision-trees"><a class="header" href="#3-decision-trees">3. Decision Trees</a></h2>
<ul>
<li><strong>ID3:</strong> Uses information gain (entropy) to split nodes.</li>
<li><strong>C4.5:</strong> Extension of ID3, handles continuous features, pruning.</li>
<li><strong>CART:</strong> Uses Gini index, supports classification and regression.</li>
<li><strong>Entropy:</strong> Measure of impurity/uncertainty.</li>
<li><strong>Information Gain:</strong> Reduction in entropy after split.</li>
<li><strong>Gini Index:</strong> Alternative impurity measure.</li>
</ul>
<h2 id="4-applications--examples"><a class="header" href="#4-applications--examples">4. Applications &amp; Examples</a></h2>
<ul>
<li><strong>Email Spam Detection:</strong> Naive Bayes for classifying emails.</li>
<li><strong>Medical Diagnosis:</strong> Decision trees for predicting diseases.</li>
<li><strong>Customer Segmentation:</strong> Unsupervised clustering.</li>
</ul>
<h2 id="5-summary"><a class="header" href="#5-summary">5. Summary</a></h2>
<ul>
<li>ML paradigms define how learning is structured.</li>
<li>Bayesian learning uses probability and prior knowledge.</li>
<li>Decision trees are interpretable, widely used for classification.</li>
</ul>
<hr />
<h1 id="-lecture-2-regression-supervised-learning--part-1"><a class="header" href="#-lecture-2-regression-supervised-learning--part-1">📘 Lecture 2: Regression (Supervised Learning – Part 1)</a></h1>
<hr />
<h2 id="-1-intuition"><a class="header" href="#-1-intuition">🔹 1. Intuition</a></h2>
<p>Regression = <strong>predicting numbers</strong>.<br />
Think of it like:</p>
<ul>
<li>Hours studied → Exam score.</li>
<li>Size of house → Price.</li>
<li>Temperature → Ice cream sales.</li>
</ul>
<p>We try to fit a <strong>line</strong> (or curve) through the data to make predictions.</p>
<hr />
<h2 id="-2-linear-regression"><a class="header" href="#-2-linear-regression">🔹 2. Linear Regression</a></h2>
<h3 id="model"><a class="header" href="#model"><strong>Model</strong></a></h3>
<p>For one variable (simple linear regression):<br />
$$
h_\theta(x) = \theta_0 + \theta_1 x
$$</p>
<ul>
<li>x: input (hours studied).</li>
<li>y: output (marks).</li>
<li>θ0: intercept (baseline).</li>
<li>θ1: slope (effect of study hours).</li>
</ul>
<hr />
<h3 id="cost-function"><a class="header" href="#cost-function"><strong>Cost Function</strong></a></h3>
<p>We want predictions close to actual values.<br />
Mean Squared Error (MSE):<br />
$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2
$$</p>
<hr />
<h3 id="optimization-gradient-descent"><a class="header" href="#optimization-gradient-descent"><strong>Optimization (Gradient Descent)</strong></a></h3>
<p>Update rule:<br />
$$
\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)
$$</p>
<ul>
<li>α: learning rate.</li>
<li>Repeat until convergence.</li>
</ul>
<hr />
<h2 id="-3-multiple-linear-regression"><a class="header" href="#-3-multiple-linear-regression">🔹 3. Multiple Linear Regression</a></h2>
<p>When there are many features:<br />
$$
h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \dots + \theta_nx_n
$$</p>
<p>Example: House price = f(size, bedrooms, location).</p>
<hr />
<h2 id="-4-logistic-regression-classification-preview"><a class="header" href="#-4-logistic-regression-classification-preview">🔹 4. Logistic Regression (Classification Preview)</a></h2>
<ul>
<li>For classification (Yes/No).</li>
<li>Uses <strong>sigmoid function</strong>:<br />
$$
h_\theta(x) = \frac{1}{1 + e^{-\theta^T x}}
$$</li>
</ul>
<hr />
<h2 id="-5-python-example-predict-exam-scores"><a class="header" href="#-5-python-example-predict-exam-scores">🔹 5. Python Example: Predict Exam Scores</a></h2>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Sample data: hours studied vs exam marks
X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)
y = np.array([40, 50, 65, 70, 85])

# Train model
model = LinearRegression()
model.fit(X, y)

# Predictions
y_pred = model.predict(X)

# Print results
print("Intercept:", model.intercept_)
print("Slope:", model.coef_)

# Plot
plt.scatter(X, y, color="blue", label="Data")
plt.plot(X, y_pred, color="red", label="Regression Line")
plt.xlabel("Hours studied")
plt.ylabel("Exam Marks")
plt.legend()
plt.show()
</code></pre>
<hr />
<h2 id="-practice-tasks-1"><a class="header" href="#-practice-tasks-1">🎯 Practice Tasks</a></h2>
<ol>
<li>
<p>Fit a linear regression line for data:<br />
Hours studied = [2, 4, 6, 8]<br />
Marks = [81, 93, 91, 97].</p>
</li>
<li>
<p>Compute cost function J(θ) for predictions [50, 60, 70] vs actual [52, 58, 75].</p>
</li>
<li>
<p>Extend to multiple regression: Predict house price using area (sqft) + bedrooms.</p>
</li>
<li>
<p>Experiment with learning rate α in gradient descent. What happens if it’s too large? Too small?</p>
</li>
<li>
<p>Plot a logistic regression curve for “exam passed” (Yes/No) given hours studied.</p>
</li>
</ol>
<hr />
<h2 id="-summary-2"><a class="header" href="#-summary-2">📝 Summary</a></h2>
<ul>
<li>Regression predicts <strong>numbers</strong> (continuous values).</li>
<li>Linear regression uses <strong>straight line</strong>: y = θ0 + θ1x.</li>
<li>Cost function = MSE.</li>
<li>Optimized using <strong>gradient descent</strong>.</li>
<li>Multiple regression → many features.</li>
<li>Logistic regression → classification (0/1).</li>
</ul>
<hr />
<div style="break-before: page; page-break-before: always;"></div><h1 id="-lecture-3-ensemble-methods"><a class="header" href="#-lecture-3-ensemble-methods">📘 Lecture 3: Ensemble Methods</a></h1>
<hr />
<h2 id="1-introduction-to-ensembles"><a class="header" href="#1-introduction-to-ensembles">1. Introduction to Ensembles</a></h2>
<ul>
<li>Why use ensembles? Bias-variance tradeoff</li>
</ul>
<h2 id="2-bagging"><a class="header" href="#2-bagging">2. Bagging</a></h2>
<ul>
<li>Bootstrap aggregating, Random Forests</li>
</ul>
<h2 id="3-boosting"><a class="header" href="#3-boosting">3. Boosting</a></h2>
<ul>
<li>AdaBoost, Gradient Boosting, XGBoost</li>
</ul>
<h2 id="4-stacking"><a class="header" href="#4-stacking">4. Stacking</a></h2>
<ul>
<li>Combining multiple models</li>
</ul>
<h2 id="5-applications--examples"><a class="header" href="#5-applications--examples">5. Applications &amp; Examples</a></h2>
<ul>
<li>Kaggle competitions, real-world use cases</li>
</ul>
<h2 id="6-summary"><a class="header" href="#6-summary">6. Summary</a></h2>
<ul>
<li>Key takeaways and further reading</li>
</ul>
<hr />
<h1 id="lecture-3-bayesian--decision-trees"><a class="header" href="#lecture-3-bayesian--decision-trees">Lecture 3: Bayesian &amp; Decision Trees</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-lecture-4-graphical-models-hmm-crf-maxent"><a class="header" href="#-lecture-4-graphical-models-hmm-crf-maxent">📘 Lecture 4: Graphical Models (HMM, CRF, MaxEnt)</a></h1>
<hr />
<h2 id="1-introduction-to-graphical-models"><a class="header" href="#1-introduction-to-graphical-models">1. Introduction to Graphical Models</a></h2>
<ul>
<li>What are graphical models? Types: directed, undirected</li>
</ul>
<h2 id="2-hidden-markov-models-hmm"><a class="header" href="#2-hidden-markov-models-hmm">2. Hidden Markov Models (HMM)</a></h2>
<ul>
<li>States, observations, transition/emission probabilities</li>
<li>Forward-backward, Viterbi algorithm</li>
</ul>
<h2 id="3-conditional-random-fields-crf"><a class="header" href="#3-conditional-random-fields-crf">3. Conditional Random Fields (CRF)</a></h2>
<ul>
<li>Sequence labeling, feature functions</li>
</ul>
<h2 id="4-maximum-entropy-models-maxent"><a class="header" href="#4-maximum-entropy-models-maxent">4. Maximum Entropy Models (MaxEnt)</a></h2>
<ul>
<li>Principle of maximum entropy, logistic regression</li>
</ul>
<h2 id="5-applications--examples-1"><a class="header" href="#5-applications--examples-1">5. Applications &amp; Examples</a></h2>
<ul>
<li>NLP, bioinformatics, time series</li>
</ul>
<h2 id="6-summary-1"><a class="header" href="#6-summary-1">6. Summary</a></h2>
<ul>
<li>Key takeaways and further reading</li>
</ul>
<hr />
<h1 id="lecture-4-ensemble-methods"><a class="header" href="#lecture-4-ensemble-methods">Lecture 4: Ensemble Methods</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fractal-ii-graphical-models--neural-networks"><a class="header" href="#fractal-ii-graphical-models--neural-networks">Fractal II: Graphical Models &amp; Neural Networks</a></h1>
<p>Overview and lecture notes for graphical models and neural networks.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lecture-5-graphical-models-hmm-crf"><a class="header" href="#lecture-5-graphical-models-hmm-crf">Lecture 5: Graphical Models (HMM, CRF)</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lecture-6-perceptron--backprop"><a class="header" href="#lecture-6-perceptron--backprop">Lecture 6: Perceptron &amp; Backprop</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lecture-7-rnn-lstm-gru"><a class="header" href="#lecture-7-rnn-lstm-gru">Lecture 7: RNN, LSTM, GRU</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lecture-8-encoder-decoder-attention-gans"><a class="header" href="#lecture-8-encoder-decoder-attention-gans">Lecture 8: Encoder-Decoder, Attention, GANs</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fractal-iii-unsupervised-learning"><a class="header" href="#fractal-iii-unsupervised-learning">Fractal III: Unsupervised Learning</a></h1>
<p>Overview and lecture notes for unsupervised learning.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lecture-9-feature-selection--pca"><a class="header" href="#lecture-9-feature-selection--pca">Lecture 9: Feature Selection &amp; PCA</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lecture-10-clustering-methods"><a class="header" href="#lecture-10-clustering-methods">Lecture 10: Clustering Methods</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lecture-11-hypothesis-evaluation"><a class="header" href="#lecture-11-hypothesis-evaluation">Lecture 11: Hypothesis Evaluation</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="assignments"><a class="header" href="#assignments">Assignments</a></h1>
<p>This section will contain assignment instructions and due dates.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="notes-in-progress"><a class="header" href="#notes-in-progress">Notes in Progress</a></h1>
<h2 id="lecture-3-sept-19--ensemble-methods"><a class="header" href="#lecture-3-sept-19--ensemble-methods">Lecture 3 (Sept 19) – Ensemble Methods</a></h2>
<ul>
<li>Bagging</li>
<li>Boosting</li>
<li>Stacking</li>
<li>Examples covered: [To be updated after class]</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
