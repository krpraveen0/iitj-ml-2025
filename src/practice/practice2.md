# üìù Practice Questions: Lecture 2

---
1. Define supervised, unsupervised, and reinforcement learning with examples.
2. What is the difference between MAP and ML estimation?
3. State and explain Bayes theorem with a real-world example.
4. How does a Naive Bayes classifier work? Give a step-by-step example.
5. What is entropy in the context of decision trees?
6. How is information gain calculated? Provide a sample calculation.
7. What is the Gini index? How is it used in CART?
8. Compare ID3, C4.5, and CART algorithms.
9. What are the advantages and disadvantages of decision trees?
10. Explain overfitting in decision trees and how to prevent it.
11. Describe the process of pruning in decision trees.
12. How do you handle missing values in decision trees?
13. Give an example of a real-world application of Naive Bayes.
14. What is the difference between classification and regression trees?
15. How can you evaluate the performance of a decision tree?
16. What is Laplace smoothing in Naive Bayes?
17. Explain the concept of conditional independence in Naive Bayes.
18. How do ensemble methods improve decision tree performance?
19. Write pseudocode for building a decision tree using ID3.
20. Discuss the limitations of Naive Bayes classifiers.

---

# üìò Lecture 2: Regression Practice Questions

---

### Q1. Simple Linear Regression
Fit a regression line for:  
- Hours studied = [2, 4, 6, 8]  
- Marks = [81, 93, 91, 97]  
Find slope and intercept.

---

### Q2. Cost Function
Compute J(Œ∏) for predictions [50, 60, 70] vs actual [52, 58, 75].

---

### Q3. Multiple Regression
House prices (in 1000s):  
- Area (sqft) = [1000, 1500, 2000]  
- Bedrooms = [2, 3, 4]  
- Prices = [200, 250, 300]  

Fit a linear model: Price = Œ∏0 + Œ∏1*Area + Œ∏2*Bedrooms.

---

### Q4. Gradient Descent
Simulate gradient descent with Œ± = 0.01, 0.1, 1.0.  
What happens to convergence?

---

### Q5. Logistic Regression Preview
Suppose exam is ‚ÄúPass‚Äù if marks ‚â• 50.  
Given study hours and results, fit a logistic regression curve.

---
