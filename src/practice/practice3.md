# üìù Practice Questions: Lecture 3

---

1. What is an ensemble method in ML?
2. Explain the bias-variance tradeoff.
3. What is bagging? How does it reduce variance?
4. Describe the Random Forest algorithm.
5. What is boosting? How does it reduce bias?
6. Explain the AdaBoost algorithm.
7. What is Gradient Boosting?
8. How does XGBoost differ from traditional boosting?
9. What is stacking in ensemble learning?
10. Give an example of a real-world application of ensemble methods.
11. How do you evaluate the performance of an ensemble?
12. What are the advantages of using ensembles?
13. What are the disadvantages of ensemble methods?
14. How do you prevent overfitting in ensembles?
15. Compare bagging and boosting.
16. What is feature importance in Random Forests?
17. How do you tune hyperparameters in ensemble models?
18. Write pseudocode for bagging.
19. Write pseudocode for boosting.
20. Discuss the role of ensembles in Kaggle competitions.

---
